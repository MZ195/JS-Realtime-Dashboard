{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcea39ba-4ef3-4468-b350-ee023450f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from random import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas import read_csv, DataFrame\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1dfd3-119d-4616-a8d8-c0694c51edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_train = read_csv('./Data/Processed_BT_PriceDB_train.csv', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
    "series_test = read_csv('./Data/Processed_BT_PriceDB_test.csv', header=0, parse_dates=[0], index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d79ed-3267-4e6d-807c-b648cd41a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(series_train.shape)\n",
    "print(series_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edab048f-6aa6-458b-b90d-264b53be6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_price_df = pd.read_csv(\"./Data/BTC_price_db.csv\")\n",
    "tweets_df = pd.read_csv(\"./Data/tweets_db.csv\")\n",
    "merge_df = pd.merge(btc_price_df,tweets_df, on='time_')\n",
    "merge_df = merge_df.drop([\"time_\"], axis=1)\n",
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0ca9f6-7f61-4e15-819a-0ddf9c4cf7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "size = int(len(merge_df)*0.7)\n",
    "\n",
    "# Divide into train and test\n",
    "train, test = merge_df[0:size], merge_df[size:len(merge_df)]\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bf18d-3a90-4a4e-b90e-dd5c35c837ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_varmax_model(train, test, varmax_order):\n",
    "    history = [x for x in train.values]\n",
    "    test = [x for x in test.values]\n",
    "    predictions = list()\n",
    "    acutal = list()\n",
    "    print(varmax_order)\n",
    "    for t in range(len(test)):\n",
    "        model = VARMAX(history, order=varmax_order)\n",
    "        model_fit = model.fit()\n",
    "        # make prediction\n",
    "        yhat = model_fit.forecast()\n",
    "        predictions.append(yhat[0][0])\n",
    "        history.append(test[t])\n",
    "        acutal.append(test[0][0])\n",
    "    rmse = sqrt(mean_squared_error(acutal, predictions))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def evaluate_models(train, test, p_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for q in q_values:\n",
    "            order = (p,q)\n",
    "            try:\n",
    "                mse = evaluate_varmax_model(train, test, order)\n",
    "                if mse < best_score:\n",
    "                    best_score, best_cfg = mse, order\n",
    "                    print('VARMAX%s RMSE=%.3f' % (order,mse))\n",
    "            except:\n",
    "                continue\n",
    "    print('Best VARMAX%s RMSE=%.3f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fcfc3-bf9b-486e-9610-265040b7a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = [0, 1, 2, 4, 6, 8, 10]\n",
    "q_values = range(0, 4)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(train, test, p_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e5d26-ac0f-4e0d-9059-a8e55d88f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrived dataset with dependency\n",
    "data = list()\n",
    "for i in range(100):\n",
    "    v1 = random()\n",
    "    v2 = v1 + random()\n",
    "    row = [v1, v2]\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0453d5e2-a64d-4794-9b45-948e6884f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mz195/Library/Python/3.8/lib/python/site-packages/statsmodels/tsa/statespace/varmax.py:161: EstimationWarning: Estimation of VARMA(p,q) models is not generically robust, due especially to identification issues.\n",
      "  warn('Estimation of VARMA(p,q) models is not generically robust,'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33423.606471755855\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = [list(x) for x in train.values]\n",
    "model = VARMAX(history, order=(1, 1))\n",
    "model_fit = model.fit(disp=False)\n",
    "# make prediction\n",
    "yhat = model_fit.forecast()\n",
    "print(yhat[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550cd90-9f36-460a-968e-4b5c938ddaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
